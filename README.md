# Interpretability-Hackathon2

This repo contains the report and code that Guillaume Corlouer and I created during Apart Research's Mechanistic Interpretability Hackathon (20/01/2023 - 22/01/2023). It incorporates significant threads of work conducted by Arthur Conmy, Neel Nanda and Redwood Research. I would like to sincerely thank them for the wealth of resources theyâ€™ve provided in support of mechanistic interpretability research. Additionally, I would like to sincerely thank Apart Research for hosting such an enjoyable hackathon.

Resources used:

* Anthropic. (2023). Transformer Circuits Thread. https://transformer-circuits.pub/

* Comney, A. (2022). Automatic Circuit Discovery Notebook.
https://colab.research.google.com/github/ArthurConmy/Easy-Transformer/blob/
main/AutomaticCircuitDiscovery.ipynb

* Comney, A. (2022). Automatic Circuit Discovery..
https://arthurconmy.github.io/automatic_circuit_discovery/

* Nanda, N. (2023). TransformerLens. https://github.com/neelnanda-io/TransformerLens

* Nanda, N. (2023). Exploratory Analysis Demo Notebook.
https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Expl
oratory_Analysis_Demo.ipynb#scrollTo=2EmZ15ejPTVo

* Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., & Carter, S. (2020). Zoom
In: An Introduction to Circuits. Distill, 5(3), 10.23915/distill.00024.001.
https://doi.org/10.23915/distill.00024.001

* Wang, K., Variengien, A., Conmy, A., Shlegeris, B., & Steinhardt, J. (2022).
Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2
Small. https://arxiv.org/pdf/2211.00593.pdf
